diff --git a/main.py b/main.py
index 8c24166..3e061d7 100644
--- a/main.py
+++ b/main.py
@@ -11,17 +11,18 @@ from transformers import (
     AutoModelForCausalLM,
     BitsAndBytesConfig,
 )
-from trl import DPOTrainer
+from trl import DPOTrainer, DPOConfig, setup_chat_format
 
 # Constants
-BASE_MODEL = "/home/zhiyu/code/llm/model/Meta-Llama-3-8B"
-DATASET_NAME = "/mnt/nasdata/yongcheng/projects/research/DPO/direct-preference-optimization/.cache/root/Anthropic___hh-rlhf"
+BASE_MODEL = "/workspace/model"
+DATASET_NAME = "/workspace/data"
 OUTPUT_DIR = "./results/"
+SAVE_DIR = "./trained_model/"
 
 # Load tokenizer and model
 tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
 torch_dtype = torch.bfloat16
-attn_implementation = "flash_attention_2"
+# attn_implementation = "flash_attention_2"
 bnb_config = BitsAndBytesConfig(
     load_in_4bit=True,
     bnb_4bit_quant_type="nf4",
@@ -33,9 +34,15 @@ model = AutoModelForCausalLM.from_pretrained(
     BASE_MODEL,
     quantization_config=bnb_config,
     device_map="auto",
-    attn_implementation=attn_implementation,
+    # attn_implementation=attn_implementation,
 )
-tokenizer.pad_token = tokenizer.eos_token
+# tokenizer.pad_token = tokenizer.eos_token
+# tokenizer.padding_side = "left"
+# tokenizer.truncation_side = "left"
+# tokenizer.padding = "max_length"
+
+
+model, tokenizer = setup_chat_format(model, tokenizer)
 
 # Load dataset
 dataset = load_from_disk(DATASET_NAME)
@@ -55,7 +62,7 @@ def split_prompt_and_responses(ex):
     return {'prompt': prompt, 'chosen': chosen_response, 'rejected': rejected_response}
 
 # Process dataset
-dataset = dataset.map(split_prompt_and_responses)
+dataset = dataset.map(split_prompt_and_responses, batched=True)
 
 # Configure LoRA
 peft_config = LoraConfig(
@@ -70,11 +77,12 @@ model.add_adapter(peft_config)
 model = prepare_model_for_kbit_training(model)
 
 # Configure training
-dpo_args = TrainingArguments(
+dpo_args = DPOConfig(
     learning_rate=1e-5,
     lr_scheduler_type="cosine",
     warmup_steps=10,
-    evaluation_strategy="steps",
+    eval_strategy="steps",
+    logging_strategy="steps",
     per_device_train_batch_size=1,
     per_device_eval_batch_size=1,
     num_train_epochs=1,
@@ -82,9 +90,10 @@ dpo_args = TrainingArguments(
     report_to="wandb",
     output_dir=OUTPUT_DIR,
     remove_unused_columns=False,
+    max_length = 1024,
+    max_prompt_length = 512,
 )
-dpo_args.max_length = 1024
-dpo_args.max_prompts_length = 512
+
 
 # Train model
 trainer = DPOTrainer(
@@ -95,4 +104,5 @@ trainer = DPOTrainer(
     eval_dataset=dataset["test"],
     data_collator=default_data_collator,
 )
-trainer.train()
\ No newline at end of file
+trainer.train()
+trainer.save_model(SAVE_DIR)
\ No newline at end of file
